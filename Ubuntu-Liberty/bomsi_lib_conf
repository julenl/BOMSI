#! /bin/bash -x

export BOMSI_LIB_CONF_RELEASE="Liberty"
export BOMSI_LIB_CONF_VERSION="0.1"
export BOMSI_LIB_CONF_OPSYS="Ubuntu"
##
## This file contains the core of BOMSI
## the functions that allow to install OpenStack with a simple command
##
##
bomsi_lib_conf_version () { 
  echo "bomsi_lib_conf version: $BOMSI_LIB_CONF_RELEASE $BOMSI_LIB_CONF_VERSION ($BOMSI_LIB_CONF_OPSYS)" 
}


if [ -e bomsi_vars ]; then
. bomsi_vars
fi


## Copy scripts and execute them on a remote machine
rcopy_execute () {
  ## Usage: rcopy_execute TARGET_IP "Commands to execute remotely"
  cat bomsi_lib_vm | sshpass -p $ROOT_PASSWORD ssh -o "StrictHostKeyChecking no" root@$1 "cat > /tmp/bomsi_lib_vm "
  cat bomsi_vars | sshpass -p $ROOT_PASSWORD ssh -o "StrictHostKeyChecking no" root@$1 "cat > /tmp/bomsi_vars "
  cat susti.py | sshpass -p $ROOT_PASSWORD ssh -o "StrictHostKeyChecking no" root@$1 "cat > /tmp/susti ; chmod 755 /tmp/susti"
  cat bomsi_lib_conf | sshpass -p $ROOT_PASSWORD ssh -o "StrictHostKeyChecking no" root@$1 "cat > /tmp/bomsi_lib_conf ; chmod 755 /tmp/bomsi_lib_conf  ; . /tmp/bomsi_{vars,lib_*}; $2" 
}


print_title (){
echo -e "\n   ################### \n   #" $1 "# \n   ################### \n"
}


log_step (){
  ## Usage: log_step 'Installing keystone'
  ## Write timestamp and sentence to /tmp/BOMSI_STEP.log
  DATE=`date +%Y-%m-%d:%H:%M:%S`
  echo $DATE " " $1 |tee -a /tmp/BOMSI_STEP.log 
}


# usage: last_step INT description_of_the_step
last_step(){
   echo "Completed until: " $1 $2 |tee -a last_step
   export LAST_STEP=$1
}
export -f last_step

if [ -f last_step ]; then
 export LAST_STEP=`tail -1 last_step | awk '{print $3}'`
else
 export LAST_STEP=0
fi


#Function to install ssh-keys
#usage: install-ssh-keys $ROOT_PASSWORD $REMOTE_IP
install-ssh-keys(){
  if [ ! -f ~/.ssh/id_rsa.pub ]; then
    echo "Creating ssh keys on local machine ($HOSTNAME)."
    ssh-keygen -f ~/.ssh/id_rsa -t rsa -N ''
  fi

   ssh-keygen -f "~/.ssh/known_hosts" -R $2
   sshpass -p $1 ssh-copy-id root@$2
#  sshpass -p $1 ssh -o 'StrictHostKeyChecking no' root@$2 'if [ ! -d ~/.ssh ]; then mkdir .ssh ; fi '
#  sshpass -p $1 scp ~/.ssh/id_rsa.pub root@$2:~/.ssh/authorized_keys
  echo "ssh keys installed"
}



host_exists () {
 ##  Usage: host_exists VM_NAME IP

 ping -w 1 -c 1 10.0.0.12 2>&1 > /dev/null  || \
 virsh domstate $VM_PREF-controlle > /dev/null 2>&1 || \
 echo "yes"
}


## Functions for network interfaces

basic_net_setup (){
 # Usage: basic_net_setup eth0 eth1

. /tmp/bomsi_vars

echo "root:$ROOT_PASSWORD"|chpasswd
sed -i 's/^PermitRootLogin .*/PermitRootLogin yes/' /etc/ssh/sshd_config

cat > /etc/network/interfaces << EOF
# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto $1
iface $1 inet static
	address 10.0.0.11
	netmask 255.255.255.0
	network 10.0.0.0
	broadcast 10.0.0.255
	gateway 10.0.0.1
	# dns-* options are implemented by the resolvconf package, if installed
	dns-nameservers 10.0.0.1

# The public network interface
auto $2
iface $2 inet manual
   up ip link set dev \$IFACE up
   down ip link set dev \$IFACE down
EOF

ifup $2

}


basic_packages (){

# These two lines remove the annoying errors on locales
sudo locale-gen de_DE.UTF-8 en_US.UTF-8
sudo dpkg-reconfigure locales

apt-get -y install chrony

# For non "controller" nodes, sync time with controller
if ! [[ $HOSTNAME =~ "controller" ]];
  then
    sed -i '0,/^server/{s/^server .*/server controller iburst/}' /etc/chrony/chrony.conf
    sed -i '/.*.debian.pool.ntp.org.*/d' /etc/chrony/chrony.conf
    service chrony restart
fi

chronyc sources

# Add repository
apt-get -y install software-properties-common
add-apt-repository -y cloud-archive:liberty

# Update
apt-get -y update && apt-get -y dist-upgrade
apt-get -y install python-openstackclient

}



mysqld_rabbitmq (){
 . /tmp/bomsi_vars

  # Install mariadb with preseeded password
  echo mysql-server mysql-server/root_password select $MYSQL_ROOT | debconf-set-selections
  echo mysql-server mysql-server/root_password_again select $MYSQL_ROOT | debconf-set-selections

  apt-get -y install mariadb-server python-pymysql

cat > /etc/mysql/conf.d/mysqld_openstack.cnf <<EOF
[mysqld]
bind-address = $CONTROLLER_IP

[mysqld]
default-storage-engine = innodb
innodb_file_per_table
collation-server = utf8_general_ci
init-connect = 'SET NAMES utf8'
character-set-server = utf8
EOF

  service mysql restart

  # Automated MySQL secure instalation
  mysql -u root -p$MYSQL_ROOT<<-EOF
UPDATE mysql.user SET Password=PASSWORD('$MYSQL_ROOT') WHERE User='root';
DELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1');
DELETE FROM mysql.user WHERE User='';
DELETE FROM mysql.db WHERE Db='test' OR Db='test\_%';
FLUSH PRIVILEGES;
EOF


  apt-get -y install rabbitmq-server

  rabbitmqctl add_user openstack $RABBIT_PASS
  rabbitmqctl set_permissions openstack ".*" ".*" ".*" 

  # check:
  #   rabbitmqctl status
  #   rabbitmqctl cluster_status
  #   rabbitmqctl list_connections
  #   rabbitmqctl list_queues |grep -v "0$"
  #   grep -i alarm /var/log/rabbitmq/* 

}





create_sql_user () {
 
. /tmp/bomsi_vars

   mysql -u root -p$MYSQL_ROOT <<-EOF
CREATE DATABASE $1;
GRANT ALL PRIVILEGES ON $1.* TO '$1'@'localhost' \
  IDENTIFIED BY '$2';
GRANT ALL PRIVILEGES ON $1.* TO '$1'@'%' \
  IDENTIFIED BY '$2';
EOF
}


install_keystone (){

 . /tmp/bomsi_{vars,lib_conf}

  ## Create the MySQL/MariaDB user for keystone
  create_sql_user keystone $KEYSTONE_DB_PASSWORD
 
  ## Generate an admin token and copy it into the variables file and to the home directory (just in case)
  if [ ! $ADMIN_TOKEN ]; then
    export ADMIN_TOKEN=$(openssl rand -hex 10 |tee token_admin)
    echo "export ADMIN_TOKEN=\"$ADMIN_TOKEN\" " >> /tmp/bomsi_vars 
  fi

  #Prevent keystone from starting automatically
  echo "manual" > /etc/init/keystone.override

  apt-get -y install keystone apache2 libapache2-mod-wsgi memcached python-memcache

  ## Edit the config file of keystone
  TMPF="/etc/keystone/keystone.conf"
  susti $TMPF DEFAULT   "admin_token =   ${ADMIN_TOKEN}"
  susti $TMPF database  "connection=mysql+pymysql://keystone:${KEYSTONE_DB_PASSWORD}@controller/keystone"
  susti $TMPF memcache   "servers = localhost:11211"
  susti $TMPF token     "provider=uuid"
  susti $TMPF token     "driver=memcache"
  susti $TMPF revoke   "driver = sql"
  susti $TMPF DEFAULT   "verbose=True"
  
  # Populate the database
  su -s /bin/sh -c "keystone-manage db_sync" keystone

  # Configure apache
  #sed -i 's/.*ServerName www.example.com:80/ServerName controller/' /etc/apache2/apache2.conf 
  echo "ServerName controller" >> /etc/apache2/apache2.conf

cat > /etc/apache2/sites-available/wsgi-keystone.conf <<EOF
Listen 5000
Listen 35357

<VirtualHost *:5000>
    WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-public
    WSGIScriptAlias / /usr/bin/keystone-wsgi-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    <IfVersion >= 2.4>
      ErrorLogFormat "%{cu}t %M"
    </IfVersion>
    ErrorLog /var/log/apache2/keystone.log
    CustomLog /var/log/apache2/keystone_access.log combined

    <Directory /usr/bin>
        <IfVersion >= 2.4>
            Require all granted
        </IfVersion>
        <IfVersion < 2.4>
            Order allow,deny
            Allow from all
        </IfVersion>
    </Directory>
</VirtualHost>

<VirtualHost *:35357>
    WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-admin
    WSGIScriptAlias / /usr/bin/keystone-wsgi-admin
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    <IfVersion >= 2.4>
      ErrorLogFormat "%{cu}t %M"
    </IfVersion>
    ErrorLog /var/log/apache2/keystone.log
    CustomLog /var/log/apache2/keystone_access.log combined

    <Directory /usr/bin>
        <IfVersion >= 2.4>
            Require all granted
        </IfVersion>
        <IfVersion < 2.4>
            Order allow,deny
            Allow from all
        </IfVersion>
    </Directory>
</VirtualHost>
EOF

  # Enable the identity services virtual hosts
  ln -s /etc/apache2/sites-available/wsgi-keystone.conf /etc/apache2/sites-enabled

  # Restart apache to use the new configuration
  service apache2 restart

  # We don't need the SQLite database created by default
  rm -f /var/lib/keystone/keystone.db


  #Cron to purge expired tokens hourly
  (crontab -l -u keystone 2>&1 | grep -q token_flush) || \
    echo '@hourly /usr/bin/keystone-manage token_flush >/var/log/keystone/keystone-tokenflush.log 2>&1' \
    >> /var/spool/cron/keystone


 ## Create the service and endpoint for keystone as well as the default project, users (admin,demo) and roles
  export OS_TOKEN=$ADMIN_TOKEN
  export OS_URL=http://controller:35357/v3
  export OS_IDENTITY_API_VERSION=3


  openstack service create \
   --name keystone --description "OpenStack Identity" identity

  openstack endpoint create --region RegionOne \
    identity public http://controller:5000/v2.0
  openstack endpoint create --region RegionOne \
    identity internal http://controller:5000/v2.0
  openstack endpoint create --region RegionOne \
    identity admin http://controller:35357/v2.0


  openstack project create --domain default \
   --description "Admin Project" admin

  openstack user create --domain default \
   --password $KEYSTONE_ADMIN_PASSWORD admin --email admin@os.mydomain.com

  openstack role create admin
  openstack role add --project admin --user admin admin

  openstack project create --domain default \
   --description "Service Project" service

  openstack project create --domain default \
   --description "Demo Project" demo
  openstack user create --domain default \
   --password $KEYSTONE_DEMO_PASSWORD demo --email demo@os.mydomain.com

  openstack role create user
  openstack role add --project demo --user demo user


  # Create the Keystone credentials file in the home directory
cat > /root/admin-openrc.sh << EOF
export OS_PROJECT_DOMAIN_ID=default
export OS_USER_DOMAIN_ID=default
export OS_PROJECT_NAME=admin
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=$KEYSTONE_ADMIN_PASSWORD
export OS_AUTH_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_REGION_NAME=RegionOne
export OS_IMAGE_API_VERSION=2
EOF

cat > ~/demo-openrc.sh << EOF
export OS_PROJECT_DOMAIN_ID=default
export OS_USER_DOMAIN_ID=default
export OS_PROJECT_NAME=demo
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=$KEYSTONE_DEMO_PASSWORD
export OS_AUTH_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_REGION_NAME=RegionOne
export OS_IMAGE_API_VERSION=2
EOF

}







install_glance (){

 . /tmp/bomsi_{vars,lib_conf}
 . ~/admin-openrc.sh

  ## Create the MySQL/MariaDB user for glance
  create_sql_user glance $GLANCE_DB_PASSWORD

 ## Create the keystone user, role and endpoint for glance
  openstack user create --domain default --password $GLANCE_PASSWORD --email glance@os.mydomain.com glance
  openstack role add --project service --user glance admin

  openstack service create --name glance \
  --description "OpenStack Image service" image

  openstack endpoint create --region RegionOne \
    image public http://controller:9292
  openstack endpoint create --region RegionOne \
    image internal http://controller:9292
  openstack endpoint create --region RegionOne \
    image admin http://controller:9292


  ## Install the packages for glance
  apt-get -y install glance python-glanceclient

  TMPF="/etc/glance/glance-api.conf"
     susti $TMPF glance_store "default_store = file"
     susti $TMPF glance_store "filesystem_store_datadir = /var/lib/glance/images/"

  for TMPF in /etc/glance/glance-api.conf /etc/glance/glance-registry.conf
   do
     susti $TMPF database   "connection = mysql+pymysql://glance:${GLANCE_DB_PASSWORD}@controller/glance"
     susti $TMPF keystone_authtoken "auth_uri = http://controller:5000"
     susti $TMPF keystone_authtoken "auth_url = http://controller:35357"
     susti $TMPF keystone_authtoken "auth_plugin = password"
     susti $TMPF keystone_authtoken "project_domain_id = default"
     susti $TMPF keystone_authtoken "user_domain_id = default"
     susti $TMPF keystone_authtoken "project_name = service"
     susti $TMPF keystone_authtoken "username = glance"
     susti $TMPF keystone_authtoken "password = $GLANCE_PASSWORD"
     susti $TMPF paste_deploy "flavor = keystone"
    
     susti $TMPF DEFAULT "notification_driver = noop"
     susti $TMPF DEFAULT "verbose = True" 
   done

  su -s /bin/sh -c "glance-manage db_sync" glance

  # Finalizes instalation
  service glance-registry restart

  # Remove the default SQLite database, which we are not going to use
  rm -f /var/lib/glance/glance.sqlite

  # Test glance

  ## if file is available localy just copy it instead of download
  if [ -f /root/LocalRepo/Packages/keystone.py ]; then
     log_step "cirros copied, not downloaded" 
     cp /root/LocalRepo/Packages/cirros-0.3.4-x86_64-disk.img /tmp/images/ 
  else
     yum -y install wget
     wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
  fi

  source ~/admin-openrc.sh

  glance image-create --name "cirros" \
  --file cirros-0.3.4-x86_64-disk.img \
  --disk-format qcow2 --container-format bare \
  --visibility public --progress  

  glance image-list

}



install_nova_controller () {

 . /tmp/bomsi_{vars,lib_conf}
 . ~/admin-openrc.sh

  create_sql_user nova $NOVA_DB_PASSWORD

  ## Create the user and endpoint for nova
  #openstack user create --domain default --password $NOVA_PASSWORD --email nova@os.mydomain.com nova
  openstack user create --project service --password $NOVA_PASSWORD --email nova@os.mydomain.com nova
  openstack role add --project service --user nova admin

 openstack service create --name nova \
  --description "OpenStack Compute" compute

  #openstack endpoint create --region RegionOne \
  #  compute public http://controller:8774/v2/%\(tenant_id\)s
  #openstack endpoint create --region RegionOne \
  #  compute internal http://controller:8774/v2/%\(tenant_id\)s
  #openstack endpoint create --region RegionOne \
  #  compute admin http://controller:8774/v2/%\(tenant_id\)s

  
 openstack endpoint create \
  --publicurl http://controller:8774/v2/%\(tenant_id\)s \
  --internalurl http://controller:8774/v2/%\(tenant_id\)s \
  --adminurl http://controller:8774/v2/%\(tenant_id\)s \
  --region RegionOne \
  compute



  # Install packages
  apt-get -y install nova-api nova-cert nova-conductor nova-consoleauth nova-novncproxy nova-scheduler python-novaclient

  ## Edit the configuration file of nova in the controller
  TMPF="/etc/nova/nova.conf"
  susti $TMPF database "connection = mysql+pymysql://nova:${NOVA_DB_PASSWORD}@controller/nova"

  susti $TMPF DEFAULT "rpc_backend = rabbit"
  susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
  susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
  susti $TMPF oslo_messaging_rabbit "rabbit_password = ${RABBIT_PASS}"
  susti $TMPF DEFAULT "auth_strategy = keystone"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000"
  susti $TMPF keystone_authtoken "auth_url = http://controller:35357"
  susti $TMPF keystone_authtoken "auth_plugin = password"
  susti $TMPF keystone_authtoken "project_domain_id = default"
  susti $TMPF keystone_authtoken "user_domain_id = default"
  susti $TMPF keystone_authtoken "project_name = service"
  susti $TMPF keystone_authtoken "username = nova"
  susti $TMPF keystone_authtoken "password = $NOVA_PASSWORD"


  MY_MGM_IP=$(ip addr show "$IFACE0" | grep inet | head -n 1 | awk '{print $2}' | cut -d/ -f1 | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')

  susti $TMPF DEFAULT "my_ip = $MY_MGM_IP"

  susti $TMPF DEFAULT "network_api_class = nova.network.neutronv2.api.API"
  susti $TMPF DEFAULT "security_group_api = neutron"
  susti $TMPF DEFAULT "linuxnet_interface_driver = nova.network.linux_net.NeutronLinuxBridgeInterfaceDriver"
  susti $TMPF DEFAULT "firewall_driver = nova.virt.firewall.NoopFirewallDriver"
  susti $TMPF vnc "vncserver_listen = \$my_ip" 
  susti $TMPF vnc "vncserver_proxyclient_address = \$my_ip" 
  susti $TMPF glance "host = controller"
  susti $TMPF oslo_concurrency "lock_path = /var/lib/nova/tmp" 
  susti $TMPF DEFAULT "enabled_apis=osapi_compute,metadata"
  susti $TMPF DEFAULT "verbose = True"

  # Populate the nova database
  su -s /bin/sh -c "nova-manage db sync" nova

  # Restart compute services
  service nova-api restart
  service nova-cert restart
  service nova-consoleauth restart
  service nova-scheduler restart
  service nova-conductor restart
  service nova-novncproxy restart

  # Delete the default SQLite database
  rm -f /var/lib/nova/nova.sqlite

}






install_nova_node (){

 . /tmp/bomsi_vars

  # Install Packages
  apt-get -y install nova-compute sysfsutils


  TMPF="/etc/nova/nova.conf"

  #rabbit_parameters $TMPF
  susti $TMPF DEFAULT "rpc_backend = rabbit"
  susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
  susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
  susti $TMPF oslo_messaging_rabbit "rabbit_password = ${RABBIT_PASS}"

  susti $TMPF DEFAULT "auth_strategy = keystone"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000"
  susti $TMPF keystone_authtoken "auth_url = http://controller:35357"
  susti $TMPF keystone_authtoken "auth_plugin = password"
  susti $TMPF keystone_authtoken "project_domain_id = default"
  susti $TMPF keystone_authtoken "user_domain_id = default"
  susti $TMPF keystone_authtoken "project_name = service"
  susti $TMPF keystone_authtoken "username = nova"
  susti $TMPF keystone_authtoken "password = $NOVA_PASSWORD"

  MY_MGM_IP=$(ip addr show "$IFACE0" | grep inet | head -n 1 | awk '{print $2}' | cut -d/ -f1 | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')

  susti $TMPF DEFAULT "my_ip = $MY_MGM_IP"

  susti $TMPF DEFAULT "network_api_class = nova.network.neutronv2.api.API"
  susti $TMPF DEFAULT "security_group_api = neutron"
  susti $TMPF DEFAULT "linuxnet_interface_driver = nova.network.linux_net.NeutronLinuxBridgeInterfaceDriver"
  susti $TMPF DEFAULT "firewall_driver = nova.virt.firewall.NoopFirewallDriver" 

  susti $TMPF vnc "enabled = True"
  susti $TMPF vnc "vncserver_listen = 0.0.0.0"
  susti $TMPF vnc "vncserver_proxyclient_address = \$my_ip"
  #susti $TMPF vnc "novncproxy_base_url = http://controller:6080/vnc_auto.html"
  susti $TMPF vnc "novncproxy_base_url = http://$CONTROLLER_IP:6080/vnc_auto.html"

  susti $TMPF glance "host = controller"
  susti $TMPF oslo_concurrency "lock_path = /var/lib/nova/tmp"
  susti $TMPF DEFAULT "verbose = True"


  TMPF="/etc/nova/nova-compute.conf"
  VCPUS=$(egrep -c '(vmx|svm)' /proc/cpuinfo)
  [ "$VCPUS" == "0" ] && susti $TMPF libvirt "virt_type = qemu"


  # Restart the compute service
  service nova-compute restart

  # Remove the SQLite database
  rm -f /var/lib/nova/nova.sqlite
  
}



check_compute () {
  . ~/admin-openrc.sh
  
 echo "### This should show 4 services in the controller and 1 in the compute, all up"
 nova service-list

 echo "### This should show 9 boxes"
 nova endpoints

 echo "### This should show the cirros image"
 nova image-list


 ssh-keygen -f ~/.ssh/id_rsa -t rsa -N ''
 nova keypair-add --pub-key ~/.ssh/id_rsa.pub demo-key
 nova keypair-list

}



















neutron_controller_provider () {
 . /tmp/bomsi_{vars,lib_conf}
 . ~/admin-openrc.sh

  apt-get -y install neutron-server neutron-plugin-ml2 \
  neutron-plugin-linuxbridge-agent neutron-dhcp-agent \
  neutron-metadata-agent python-neutronclient conntrack

  
  TMPF="/etc/neutron/neutron.conf"
  susti $TMPF database "connection = mysql+pymysql://neutron:${NEUTRON_DB_PASSWORD}@controller/neutron"

  susti $TMPF DEFAULT "core_plugin = ml2"
  susti $TMPF DEFAULT "service_plugins = "
  susti $TMPF DEFAULT "allow_overlapping_ips = True"

  susti $TMPF DEFAULT "rpc_backend = rabbit"
  susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
  susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
  susti $TMPF oslo_messaging_rabbit "rabbit_password = ${RABBIT_PASS}"

  susti $TMPF DEFAULT "auth_strategy = keystone"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000"
  susti $TMPF keystone_authtoken "auth_url = http://controller:35357"
  susti $TMPF keystone_authtoken "auth_plugin = password"
  susti $TMPF keystone_authtoken "project_domain_id = default"
  susti $TMPF keystone_authtoken "user_domain_id = default"
  susti $TMPF keystone_authtoken "project_name = service"
  susti $TMPF keystone_authtoken "username = neutron"
  susti $TMPF keystone_authtoken "password = $NEUTRON_PASSWORD"

  susti $TMPF DEFAULT "notify_nova_on_port_status_changes = True"
  susti $TMPF DEFAULT "notify_nova_on_port_data_changes = True"
  susti $TMPF DEFAULT "nova_url = http://controller:8774/v2"
  susti $TMPF nova "auth_url = http://controller:35357"
  susti $TMPF nova "auth_plugin = password"
  susti $TMPF nova "project_domain_id = default"
  susti $TMPF nova "user_domain_id = default"
  susti $TMPF nova "region_name = RegionOne"
  susti $TMPF nova "project_name = service"
  susti $TMPF nova "username = nova"
  susti $TMPF nova "password = $NOVA_PASSWORD"
  susti $TMPF DEFAULT "verbose = True"


  TMPF="/etc/neutron/plugins/ml2/ml2_conf.ini"
  susti $TMPF ml2  "type_drivers = flat,vlan"
  susti $TMPF ml2  "tenant_network_types = "
  susti $TMPF ml2  "mechanism_drivers = linuxbridge"

  #susti $TMPF ml2  "extension_drivers = port_security"
  #susti $TMPF ml2_type_flat  "flat_networks = public"
  #susti $TMPF ml2_type_vxlan  "vni_ranges = 1:1000"
  #susti $TMPF securitygroup "enable_ipset = True"

  TMPF="/etc/neutron/plugins/ml2/linuxbridge_agent.ini"
  susti $TMPF linux_bridge "physical_interface_mappings = public:$IFACE1" #IFACE_EXT
  susti $TMPF vxlan "enable_vxlan = False"
  #susti $TMPF vxlan "local_ip = OVERLAY_INTERFACE_IP_ADDRESS"
  #susti $TMPF vxlan "el2_population = True"
  susti $TMPF agent "prevent_arp_spoofing = True" 
  susti $TMPF securitygroup "enable_security_group = True"
  susti $TMPF securitygroup "firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver"
 
  TMPF="/etc/neutron/dhcp_agent.ini"
  susti $TMPF DEFAULT "interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver"
  susti $TMPF DEFAULT "dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq"
  susti $TMPF DEFAULT "enable_isolated_metadata = True"
  susti $TMPF DEFAULT "verbose = True"
  #susti $TMPF DEFAULT "dnsmasq_config_file = /etc/neutron/dnsmasq-neutron.conf"
  

}



neutron_controller_self-serv () {
 . /tmp/bomsi_{vars,lib_conf}
 . ~/admin-openrc.sh

  # Install exactly the same packages as in "provider network" section
  apt-get -y install neutron-server neutron-plugin-ml2 \
  neutron-plugin-linuxbridge-agent neutron-l3-agent neutron-dhcp-agent \
  neutron-metadata-agent python-neutronclient conntrack

   TMPF="/etc/neutron/neutron.conf"
  susti $TMPF database "connection = mysql+pymysql://neutron:${NEUTRON_DB_PASSWORD}@controller/neutron"

  susti $TMPF DEFAULT "core_plugin = ml2"
  susti $TMPF DEFAULT "service_plugins = router"  ## This is different
  susti $TMPF DEFAULT "allow_overlapping_ips = True"

  susti $TMPF DEFAULT "rpc_backend = rabbit"
  susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
  susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
  susti $TMPF oslo_messaging_rabbit "rabbit_password = ${RABBIT_PASS}"

  susti $TMPF DEFAULT "auth_strategy = keystone"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000"
  susti $TMPF keystone_authtoken "auth_url = http://controller:35357"
  susti $TMPF keystone_authtoken "auth_plugin = password"
  susti $TMPF keystone_authtoken "project_domain_id = default"
  susti $TMPF keystone_authtoken "user_domain_id = default"
  susti $TMPF keystone_authtoken "project_name = service"
  susti $TMPF keystone_authtoken "username = neutron"
  susti $TMPF keystone_authtoken "password = $NEUTRON_PASSWORD"

  susti $TMPF DEFAULT "notify_nova_on_port_status_changes = True"
  susti $TMPF DEFAULT "notify_nova_on_port_data_changes = True"
  susti $TMPF DEFAULT "nova_url = http://controller:8774/v2"
  susti $TMPF nova "auth_url = http://controller:35357"
  susti $TMPF nova "auth_plugin = password"
  susti $TMPF nova "project_domain_id = default"
  susti $TMPF nova "user_domain_id = default"
  susti $TMPF nova "region_name = RegionOne"
  susti $TMPF nova "project_name = service"
  susti $TMPF nova "username = nova"
  susti $TMPF nova "password = $NOVA_PASSWORD"
  susti $TMPF DEFAULT "verbose = True"


  TMPF="/etc/neutron/plugins/ml2/ml2_conf.ini"
  susti $TMPF ml2  "type_drivers = flat,vlan,vxlan"  ## This is different
  susti $TMPF ml2  "tenant_network_types = vxlan"  ## This is different
  susti $TMPF ml2  "mechanism_drivers = linuxbridge,l2population" ## This is different
  susti $TMPF ml2  "extension_drivers = port_security"  ## This is different
  susti $TMPF ml2_type_flat "flat_networks = public"  ## This is different
  susti $TMPF ml2_type_vxlan "vni_ranges = 1:1000"
  susti $TMPF securitygroup "enable_ipset = True"


  TMPF="/etc/neutron/plugins/ml2/linuxbridge_agent.ini"
  susti $TMPF linux_bridge "physical_interface_mappings = public:$IFACE1" #IFACE_EXT
  susti $TMPF vxlan "enable_vxlan = True"  ## This is different

  MY_MGM_IP=$(ip addr show "$IFACE0" | grep inet | head -n 1 | awk '{print $2}' | cut -d/ -f1 | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')

  susti $TMPF vxlan "local_ip = $MY_MGM_IP" ## This is different
  susti $TMPF vxlan "l2_population = True"  ## This is different
  susti $TMPF agent "prevent_arp_spoofing = True" 
  susti $TMPF securitygroup "enable_security_group = True"
  susti $TMPF securitygroup "firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver"

  TMPF="/etc/neutron/l3_agent.ini" 
  susti $TMPF DEFAULT "interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver"
  susti $TMPF DEFAULT "external_network_bridge = "
  susti $TMPF DEFAULT "verbose = True"

 
  TMPF="/etc/neutron/dhcp_agent.ini"
  susti $TMPF DEFAULT "interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver"
  susti $TMPF DEFAULT "dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq"
  susti $TMPF DEFAULT "enable_isolated_metadata = True"
  susti $TMPF DEFAULT "verbose = True"
  susti $TMPF DEFAULT "dnsmasq_config_file = /etc/neutron/dnsmasq-neutron.conf"
  
  echo "dhcp-option-force=26,1450" > /etc/neutron/dnsmasq-neutron.conf

}






install_neutron_controller () {
 print_title "install_neutron_controller"

 . /tmp/bomsi_{vars,lib_conf}
 . ~/admin-openrc.sh

  create_sql_user neutron $NEUTRON_DB_PASSWORD


  openstack user create --project service --password $NEUTRON_PASSWORD --email neutron@os.mydomain.com neutron
  openstack role add --project service --user neutron admin

  openstack service create --name neutron \
  --description "OpenStack Networking" network
  
  openstack endpoint create \
  --publicurl http://controller:9696 \
  --internalurl http://controller:9696 \
  --adminurl http://controller:9696 \
  --region RegionOne \
  network


  #neutron_controller_provider
  neutron_controller_self-serv 


  TMPF="/etc/neutron/metadata_agent.ini"
  susti $TMPF DEFAULT "auth_url = http://controller:5000"
  susti $TMPF DEFAULT "auth_url = http://controller:35357"
  susti $TMPF DEFAULT "auth_region = RegionOne"
  susti $TMPF DEFAULT "auth_plugin = password"
  susti $TMPF DEFAULT "project_domain_id = default"
  susti $TMPF DEFAULT "user_domain_id = default"
  susti $TMPF DEFAULT "project_name = service"
  susti $TMPF DEFAULT "username = neutron"
  susti $TMPF DEFAULT "password = $NEUTRON_PASSWORD"
  susti $TMPF DEFAULT "nova_metadata_ip = controller"
  susti $TMPF DEFAULT "metadata_proxy_shared_secret = $METADATA_SECRET"
  susti $TMPF DEFAULT "verbose = True"


  TMPF="/etc/nova/nova.conf"
  susti $TMPF neutron "url = http://controller:9696"
  susti $TMPF neutron "auth_url = http://controller:35357"
  susti $TMPF neutron "auth_plugin = password"
  susti $TMPF neutron "project_domain_id = default"
  susti $TMPF neutron "user_domain_id = default"
  susti $TMPF neutron "region_name = RegionOne"
  susti $TMPF neutron "project_name = service"
  susti $TMPF neutron "username = neutron"
  susti $TMPF neutron "password = $NEUTRON_PASSWORD"

  susti $TMPF neutron  "service_metadata_proxy = True"
  susti $TMPF neutron  "metadata_proxy_shared_secret = $METADATA_SECRET"

  # Populate the database
  su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
  --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron

  # Restart the services
  service nova-api restart
  service neutron-server restart
  service neutron-plugin-linuxbridge-agent restart
  service neutron-dhcp-agent restart
  service neutron-metadata-agent restart

  service neutron-l3-agent restart

  # Remove the SQLite database
  rm -f /var/lib/neutron/neutron.sqlite


}





install_neutron_node_provider () {

 . /tmp/bomsi_{vars,lib_conf}

  TMPF="/etc/neutron/plugins/ml2/linuxbridge_agent.ini"
  susti $TMPF linux_bridge "physical_interface_mappings = public:$IFACE1" #IFACE_EXT
  susti $TMPF vxlan "enable_vxlan = False"
  susti $TMPF agent "prevent_arp_spoofing = True"
  susti $TMPF securitygroup "enable_security_group = True" 
  susti $TMPF securitygroup "firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver" 
}


install_neutron_node_self-serv () {

 . /tmp/bomsi_{vars,lib_conf}

  TMPF="/etc/neutron/plugins/ml2/linuxbridge_agent.ini"
  susti $TMPF linux_bridge "physical_interface_mappings = public:$IFACE1" #IFACE_EXT
  susti $TMPF vxlan "enable_vxlan = False"
  susti $TMPF agent "prevent_arp_spoofing = True"
  MY_MGM_IP=$(ip addr show "$IFACE0" | grep inet | head -n 1 | awk '{print $2}' | cut -d/ -f1 | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')

  susti $TMPF vxlan "local_ip = $MY_MGM_IP" ## This is different
  susti $TMPF vxlan "el2_population = True"  ## This is different

  susti $TMPF securitygroup "enable_security_group = True" 
  susti $TMPF securitygroup "firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver" 
}



install_neutron_node () {

 . /tmp/bomsi_{vars,lib_conf}
   print_title "install_neutron_node"

  # Install packages
  apt-get -y install neutron-plugin-linuxbridge-agent conntrack

  
   ## This part here is exactly the same as in the controller
  TMPF="/etc/neutron/neutron.conf"

  susti $TMPF DEFAULT "rpc_backend = rabbit"
  susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
  susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
  susti $TMPF oslo_messaging_rabbit "rabbit_password = ${RABBIT_PASS}"

  susti $TMPF DEFAULT "auth_strategy = keystone"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000"
  susti $TMPF keystone_authtoken "auth_url = http://controller:35357"
  susti $TMPF keystone_authtoken "auth_plugin = password"
  susti $TMPF keystone_authtoken "project_domain_id = default"
  susti $TMPF keystone_authtoken "user_domain_id = default"
  susti $TMPF keystone_authtoken "project_name = service"
  susti $TMPF keystone_authtoken "username = neutron"
  susti $TMPF keystone_authtoken "password = $NEUTRON_PASSWORD"
  susti $TMPF DEFAULT "core_plugin = ml2"
  susti $TMPF DEFAULT "service_plugins = router"
  susti $TMPF DEFAULT "allow_overlapping_ips = True"
  susti $TMPF DEFAULT "verbose = True"


  #install_neutron_node_provider
  install_neutron_node_self-serv


  TMPF="/etc/nova/nova.conf"
  susti $TMPF neutron "neutronurl = http://controller:9696"
  susti $TMPF neutron "neutronauth_url = http://controller:35357"
  susti $TMPF neutron "neutronauth_plugin = password"
  susti $TMPF neutron "neutronproject_domain_id = default"
  susti $TMPF neutron "neutronuser_domain_id = default"
  susti $TMPF neutron "neutronregion_name = RegionOne"
  susti $TMPF neutron "neutronproject_name = service"
  susti $TMPF neutron "neutronusername = neutron"
  susti $TMPF neutron "neutronpassword = ${NEUTRON_PASSWORD}"

  service nova-compute restart
  service neutron-plugin-linuxbridge-agent restart

}






create_neutron_networks () {

 . /tmp/bomsi_vars
 . ~/admin-openrc.sh

  #echo "### This should show 5 entries"
  #neutron agent-list

  neutron net-create public --shared --provider:physical_network public \
  --provider:network_type flat


 export PUBLIC_NETWORK_CIDR=$IPPR_A'0/24' #Network
 export START_IP_ADDRESS=$IPPR_A'80'  #1st IP in range
 export END_IP_ADDRESS=$IPPR_A'90'  #last IP
 export DNS_RESOLVER="8.8.4.4"
 export PUBLIC_NETWORK_GATEWAY=$GATEWAY  #Gateway


  neutron subnet-create public $PUBLIC_NETWORK_CIDR --name public \
    --allocation-pool start=$START_IP_ADDRESS,end=$END_IP_ADDRESS\
    --dns-nameserver $DNS_RESOLVER --gateway $PUBLIC_NETWORK_GATEWAY


  neutron net-create private

  export PRIVATE_NETWORK_CIDR="192.168.1.0/24"
  export DNS_RESOLVER="8.8.4.4"
  export PRIVATE_NETWORK_GATEWAY=192.168.1.1

  neutron subnet-create private $PRIVATE_NETWORK_CIDR --name private \
  --dns-nameserver $DNS_RESOLVER --gateway $PRIVATE_NETWORK_GATEWAY


  neutron net-update public --router:external

  . ~/demo-openrc.sh
  neutron router-create router
  . ~/admin-openrc.sh
  neutron router-interface-add router private
  neutron router-gateway-set router public

 #Delete those networks and routers
# neutron router-gateway-clear router && neutron router-interface-delete router private && neutron router-delete router && neutron net-delete private && neutron net-delete public


 #ifconfig $IFACE0 promisc

 print_title "Testing Neutron Networkin"

 echo "Check that l3-agent created the networks properly"
 QROUTER=$(ip netns | grep qrouter-)
 ip netns exec $QROUTER ip addr list
 echo "Ping Gateway of the external network"
 ip netns exec $QROUTER ping -c3 $START_IP_ADDRESS

 neutron port-list
 #neutron port-show #ID

 #To delete them
 #dhcp_delete_namespaces
 #dhcp_delete_namespaces


 # Security groups
 nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0
 nova secgroup-add-rule default tcp 22 22 0.0.0.0/0

 # To launch an instance
 #NET_ID=$(neutron net-list |grep private |awk '{print $2}')
 #nova boot --flavor m1.tiny --image cirros --nic net-id=$NET_ID --security-group default --key-name demo-key private-instance

}




install_horizon () {

 . /tmp/bomsi_vars

  apt-get -y install openstack-dashboard

  TMPF="/etc/openstack-dashboard/local_settings.py"
  sed -i "s/.*OPENSTACK_HOST =.*/OPENSTACK_HOST = \"controller\"/" $TMPF
  sed -i "s/.*ALLOWED_HOSTS =.*/ALLOWED_HOSTS = ['*', ]/" $TMPF

  ###Bugfix
  #TMPF="/usr/share/openstack-dashboard/openstack_dashboard/settings.py"
  #sed -i "s/MESSAGE_STORAGE = 'django.contrib.messages.storage.fallback.FallbackStorage'.*/\
  #MESSAGE_STORAGE = 'django.contrib.messages.storage.fallback.FallbackStorage' \n\
  #AUTH_USER_MODEL = 'openstack_auth.User'/" $TMPF

  #sed -i "s/.*'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',.*/\
  #      'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',\n\
  #      'LOCATION': '127.0.0.1:11211', /" $TMPF

  sed -i "s/OPENSTACK_KEYSTONE_DEFAULT_ROLE =.*/OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"user\"/" $TMPF
  
  # Restart apache
  service apache2 reload

}









































## Function bunddles for each machine

controller_bund () {
 ## This function is just a bunddle of installer functions, for a cleaner code
 . /tmp/bomsi_{vars,lib_*}
  
  log_step 'Executing controller_bund'
  log_step 'Executing mysqld_rabbitmq'
  mysqld_rabbitmq &> /tmp/C01.inst_rabbit.log
  log_step 'Executing install_keystone'
  install_keystone &> /tmp/C02.inst_keystone.log
  log_step 'Executing install_glance'
  install_glance &> /tmp/C03.inst_glance.log
  log_step 'Executing install_nova_controller'
  install_nova_controller &> /tmp/A04.inst_nova_controller.log
  log_step 'Executing install_neutron_controller'
  install_neutron_controller &> /tmp/A05.inst_neutron_controller.log
  log_step 'Executing install_horizon'
  install_horizon &> /tmp/A06.inst_horizon.log 

  #The next part is released as a child script
  # and executed when the first nova node shows up
  log_step 'Generating script to wait for compute1 and execute the rest'
  cat >/tmp/controller_finish_compute.sh<<EOF 
#! /bin/bash
  . /tmp/bomsi_{vars,lib_*}
  . /root/admin-openrc.sh
  . /tmp/bomsi_lib_conf
  log_step 'Creating /tmp/controller_finish_compute.sh '
  while ! nova service-list |grep nova-compute 
    do
      sleep 10 && log_step 'waiting for compute1 node' >> /tmp/waiting4compute.log
    done

  check_compute > /tmp/check_compute.log
  log_step 'Compute service installation finished'
EOF
  log_step 'Executing /tmp/controller_finish_compute.sh'
  chmod +x /tmp/controller_finish_compute.sh
  nohup /tmp/controller_finish_compute.sh  

  # The next child script is executed when ovs at neutron nodecomes up
  cat >/tmp/controller_finish_network.sh<<EOF 
#! /bin/bash
  . /tmp/bomsi_{vars,lib_*}
  . /root/admin-openrc.sh
  . /tmp/bomsi_lib_conf
  log_step 'Creating /tmp/controller_finish_neutron.sh '
  #while ! neutron agent-list |grep neutron-openvswitch-agent 
  #  do
  #    sleep 10 && log_step 'waiting for network node' >> /tmp/waiting4compute.log
  #  done


#  install_neutron_node_meta_nova &> /tmp/C07.install_neutron_node_meta 
#  check_openvswitch &> /tmp/C08.check_openvswitch.log 
  create_neutron_networks &> /tmp/C09.create_neutron_netw.log
  log_step 'Network service installation finished'
EOF
  log_step 'Executing /tmp/controller_finish_network.sh'
  chmod +x /tmp/controller_finish_network.sh
  nohup /tmp/controller_finish_network.sh 
      

  #This will send create and execute a heat template once everything else is up
  cat >/tmp/controller_finish_heat.sh<<EOF 
#! /bin/bash
  . /tmp/bomsi_{vars,lib_*}
  . /root/admin-openrc.sh
  . /tmp/bomsi_lib_conf
  log_step 'Creating /tmp/controller_finish_heat.sh '
  while ! neutron agent-list |grep neutron-openvswitch-agent 
    do
      sleep 10 && log_step 'waiting for network node for heat' >> /tmp/waiting4compute.log
    done
  while ! nova service-list |grep nova-compute 
    do
      sleep 10 && log_step 'waiting for compute1 node for heat' >> /tmp/waiting4compute.log
    done

  install_heat && heat_template > /tmp/heat_template.log

  log_step 'Orchestation (heat) service installation finished'
EOF
  log_step 'Executing /tmp/controller_finish_heat.sh'
  chmod +x /tmp/controller_finish_heat.sh
  nohup /tmp/controller_finish_heat.sh 
}













install_cinder_controller () {
 
 . /tmp/bomsi_{vars,lib_conf}
  #. /tmp/bomsi_vars
  #. /tmp/bomsi_lib_conf
 . ~/admin-openrc.sh

  create_sql_user cinder $CINDER_DB_PASSWORD

  openstack user create --password $CINDER_PASSWORD --email cinder@os.dkrz.de cinder
  openstack role add --project service --user cinder admin

  openstack service create --name cinder \
  --description "OpenStack Block Storage" volume

  openstack service create --name cinderv2 \
  --description "OpenStack Block Storage" volumev2

  CINDER_EP=block

  openstack endpoint create \
  --publicurl http://$CINDER_EP:8776/v2/%\(tenant_id\)s \
  --internalurl http://$CINDER_EP:8776/v2/%\(tenant_id\)s \
  --adminurl http://$CINDER_EP:8776/v2/%\(tenant_id\)s \
  --region RegionOne \
  volume

  openstack endpoint create \
  --publicurl http://$CINDER_EP:8776/v2/%\(tenant_id\)s \
  --internalurl http://$CINDER_EP:8776/v2/%\(tenant_id\)s \
  --adminurl http://$CINDER_EP:8776/v2/%\(tenant_id\)s \
  --region RegionOne \
  volumev2

  yum -y install openstack-cinder python-cinderclient python-oslo-db

  unalias cp
  cp /usr/share/cinder/cinder-dist.conf /etc/cinder/cinder.conf
  chown -R cinder:cinder /etc/cinder/cinder.conf

  TMPF="/etc/cinder/cinder.conf"
  susti $TMPF database "connection = mysql://cinder:$CINDER_DB_PASSWORD@controller/cinder"
  rabbit_parameters $TMPF

   #susti $TMPF DEFAULT "rpc_backend = rabbit"
   #susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
   #susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
   #susti $TMPF oslo_messaging_rabbit "rabbit_password = $RABBIT_PASS"
  susti $TMPF DEFAULT "auth_strategy = keystone"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000"
  susti $TMPF keystone_authtoken "auth_url = http://controller:35357"
  susti $TMPF keystone_authtoken "auth_plugin = password"
  susti $TMPF keystone_authtoken "project_domain_id = default"
  susti $TMPF keystone_authtoken "user_domain_id = default"
  susti $TMPF keystone_authtoken "project_name = service"
  susti $TMPF keystone_authtoken "username = cinder"
  susti $TMPF keystone_authtoken "password = $CINDER_PASSWORD"
   #Comment out the rest of the stuff in the section
  sed -i '/identity_uri/s/^/#/g' $TMPF
  sed -i '/admin_tenant_name/s/^/#/g' $TMPF
  sed -i '/admin_user/s/^/#/g' $TMPF
  sed -i '/admin_password/s/^/#/g' $TMPF
  #susti $TMPF DEFAULT "verbose = True"
  #MY_MGM_IP=`LC_ALL=C ip addr show "$IFACE0" | grep inet | head -n 1 | awk '{print $2}' | cut -d/ -f1 | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$'`
 
   MY_MGM_IP=$(ip addr show "$IFACE0" | grep inet | head -n 1 | awk '{print $2}' | cut -d/ -f1 | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')
 
  susti $TMPF DEFAULT "my_ip = $MY_MGM_IP"

  #susti $TMPF DEFAULT "my_ip = $CONTROLLER_IP"
  susti $TMPF oslo_concurrency "lock_path = /var/lock/cinder"
  susti $TMPF DEFAULT "verbose = True"


  su -s /bin/sh -c "cinder-manage db sync" cinder
 
  systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service
  systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service

  echo "export OS_VOLUME_API_VERSION=2" | tee -a admin-openrc.sh demo-openrc.sh

}



install_cinder_node () {

 . /tmp/bomsi_vars
 #. ~/openrca.sh

  yum -y install lvm2

  systemctl enable lvm2-lvmetad.service
  systemctl start lvm2-lvmetad.service
 
  DISK="vdb"
  DISK_PATH="/dev/"$DISK
  #Partition disk
  echo -e "o\nn\np\n1\n\n\nw" | fdisk $DISK_PATH
  #/dev/vda2  1026048  20971519  86  Linux LVM
  #Create LVM
  pvcreate $DISK_PATH"1"
  vgcreate cinder-volumes $DISK_PATH"1"

  #TMPF="/etc/lvm/lvm.conf"
  #sed -i 's/   filter = \[ \"a\/.*\/\" \]/   filter = [ "a\/vda\/", "a\/vdb\/", "r\/.*\/"]/' $TMPF

  yum -y install openstack-cinder targetcli python-oslo-db python-oslo-log MySQL-python
  #yum -y install openstack-cinder targetcli python-oslo-db MySQL-python

  TMPF="/etc/cinder/cinder.conf"
  susti $TMPF database "connection = mysql://cinder:$CINDER_DB_PASSWORD@controller/cinder"

  rabbit_parameters $TMPF
   #susti $TMPF DEFAULT "rpc_backend = rabbit"
   #susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
   #susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
   #susti $TMPF oslo_messaging_rabbit "rabbit_password = $RABBIT_PASS"
  susti $TMPF DEFAULT "auth_strategy = keystone"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000"
  susti $TMPF keystone_authtoken "auth_url = http://controller:35357"
  susti $TMPF keystone_authtoken "auth_plugin = password"
  susti $TMPF keystone_authtoken "project_domain_id = default"
  susti $TMPF keystone_authtoken "user_domain_id = default"
  susti $TMPF keystone_authtoken "project_name = service"
  susti $TMPF keystone_authtoken "username = cinder"
  susti $TMPF keystone_authtoken "password = $CINDER_PASSWORD"

  #susti $TMPF DEFAULT "my_ip = $CONTROLLER_IP"
   ## Find the local IP in the Management net (assuming it is in $IFACE0 interface)
    MY_MGM_IP=$(ip addr show "$IFACE0" | grep inet | head -n 1 | awk '{print $2}' | cut -d/ -f1 | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')

  cat > /etc/rsyncd.conf<<EOF
uid = swift
gid = swift
log file = /var/log/rsyncd.log
pid file = /var/run/rsyncd.pid
address = $MY_MGM_IP
[account]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/account.lock
[container]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/container.lock
[object]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/object.lock
EOF

  susti $TMPF DEFAULT "my_ip = $MY_MGM_IP"

  susti $TMPF lvm "volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver"
  susti $TMPF lvm "volume_group = cinder-volumes"
  susti $TMPF lvm "iscsi_protocol = iscsi"
  susti $TMPF lvm "iscsi_helper = lioadm"
  susti $TMPF DEFAULT "enabled_backends = lvm"
  susti $TMPF DEFAULT "glance_host = controller"
  susti $TMPF oslo_concurrency "lock_path = /var/lock/cinder"
  susti $TMPF DEFAULT "verbose = True"

  mkdir /var/lock/cinder
  chmod -R 777 /var/lock/cinder

  systemctl enable openstack-cinder-volume.service target.service
  systemctl start openstack-cinder-volume.service target.service

  #Usefull commands: vgdisplay  &&  lvscan

}

cinder_test () {
  source ~/admin-openrc.sh 
  cinder service-list
  cinder create --display-name demo-volume1 1
  cinder list
}



install_swift_controller () {

 . /tmp/bomsi_vars
 . ~/admin-openrc.sh

  openstack user create --password $SWIFT_PASSWORD --email swift@os.dkrz.de swift
  openstack role add --project service --user swift admin

   openstack service create --name swift \
  --description "OpenStack Object Storage" object-store

  openstack endpoint create \
  --publicurl 'http://controller:8080/v1/AUTH_%(tenant_id)s' \
  --internalurl 'http://controller:8080/v1/AUTH_%(tenant_id)s' \
  --adminurl http://controller:8080 \
  --region RegionOne \
  object-store

  yum -y install openstack-swift-proxy python-swiftclient python-keystone-auth-token \
  python-keystonemiddleware memcached

  
  rm -rf /etc/swift/proxy-server.conf
  if [ -f /root/LocalRepo/Packages/account-server.conf-sample ]; then
     cp /root/LocalRepo/Packages/proxy-server.conf-sample /etc/swift/proxy-server.conf 
     echo " " > /root/swiftprox-conf-files-notdl
  else
     curl -o /etc/swift/proxy-server.conf \
     https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/kilo
  fi


  
  TMPF="/etc/swift/proxy-server.conf"
  susti $TMPF DEFAULT "bind_port = 8080" 
  susti $TMPF DEFAULT "user = swift"
  susti $TMPF DEFAULT "swift_dir=/etc/swift"
  #susti $TMPF "pipeline:main" "pipeline = authtoken cache healthcheck keystoneauth proxy-logging proxy-server" 
  susti $TMPF "pipeline:main" "pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo proxy-logging proxy-server" 
  #susti $TMPF "app:proxy-server" "allow_account_management = true" 
  susti $TMPF "app:proxy-server" "account_autocreate = true" 
  susti $TMPF "filter:keystoneauth" "use = egg:swift#keystoneauth" 
  #susti $TMPF "filter:keystoneauth" "operator_roles = admin,_member_" 
  susti $TMPF "filter:keystoneauth" "operator_roles = admin,user" 
  susti $TMPF "filter:authtoken" "paste.filter_factory = keystonemiddleware.auth_token:filter_factory" 
  susti $TMPF "filter:authtoken" "auth_uri = http://controller:5000" 
  susti $TMPF "filter:authtoken" "auth_url = http://controller:35357" 
  susti $TMPF "filter:authtoken" "auth_plugin = password" 
  susti $TMPF "filter:authtoken" "project_domain_id = default" 
  susti $TMPF "filter:authtoken" "user_domain_id = default" 
  susti $TMPF "filter:authtoken" "project_name = service" 
  susti $TMPF "filter:authtoken" "username = swift" 
  susti $TMPF "filter:authtoken" "password = $SWIFT_PASSWORD" 
  susti $TMPF "filter:authtoken" "delay_auth_decision = true" 
  susti $TMPF "filter:cache" "memcache_servers = 127.0.0.1:11211" 

}




install_swift_node () {

 . /tmp/bomsi_{vars,lib_conf}
 #. /tmp/bomsi_vars
 #. /tmp/bomsi_lib_conf
 #. ~/openrca.sh

  yum -y install xfsprogs rsync

  selinux_firewall_off

  ## Try to get all disks available. Assuming the filesystem is on /dev/*da
  ## A better solution would probably use lsblk -il or something
  FREE_DISKS=$(fdisk -l |grep Disk |awk '{print $2}' |grep '^/dev/[hsv]d..$' |grep -v "a"|sed 's/://'|awk -F"/" '{print $NF}')
  echo $FREE_DISKS

  ## Prepare all free disks and mount them in /srv/node
  for DISK in $FREE_DISKS
    do
      echo -e "o\nn\np\n1\n\n\nw" | fdisk /dev/$DISK
      mkfs.xfs -f /dev/$DISK"1"
      mkdir -p /srv/node/$DISK"1"
      #Add line to fstab only if it does not exist!
      grep -q "/dev/${DISK}1 /srv/node/${DISK}1"  /etc/fstab || \
      echo "/dev/${DISK}1 /srv/node/${DISK}1 xfs noatime,nodiratime,nobarrier,logbufs=8 0 2" >> /etc/fstab
      mount /srv/node/$DISK"1"
    done

  ## If the object node has only one disk, attach a "virtual" one of 2Gb
  if [ -z  $FREE_DISKS ]; then
    dd if=/dev/zero of=/root/DISK1.img bs=1M count=2000
    mkfs.xfs /root/DISK1.img
    echo "/root/DISK1 /srv/node/vfda1 xfs noatime,nodiratime,nobarrier,logbufs=8 0 2" >> /etc/fstab
    mount /srv/node/vfda1
    FREE_DISKS="vfda1"
  fi

 echo $FREE_DISKS > ~/FREE_DISKS

 #DISK1="vdb"
 #DISK2="vdc"

 ##Partition disk
 #echo -e "o\nn\np\n1\n\n\nw" | fdisk /dev/$DISK1
 #echo -e "o\nn\np\n1\n\n\nw" | fdisk /dev/$DISK2

 #mkfs.xfs /dev/$DISK1"1"
 #mkfs.xfs /dev/$DISK2"1"

 #mkdir -p /srv/node/$DISK1"1"
 #mkdir -p /srv/node/$DISK2"1"

 #echo "/dev/"$DISK1"1 /srv/node/$DISK1"1" xfs noatime,nodiratime,nobarrier,logbufs=8 0 2" >> /etc/fstab
 #echo "/dev/"$DISK2"1 /srv/node/$DISK2"1" xfs noatime,nodiratime,nobarrier,logbufs=8 0 2" >> /etc/fstab

 #mount /srv/node/$DISK1"1"
 #mount /srv/node/$DISK2"1"


  MY_MGM_IP=$(ip addr show "$IFACE0" | grep inet | head -n 1 | awk '{print $2}' | cut -d/ -f1 | grep -E '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$')


  cat > /etc/rsyncd.conf<<EOF
uid = swift
gid = swift
log file = /var/log/rsyncd.log
pid file = /var/run/rsyncd.pid
address = $MY_MGM_IP
[account]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/account.lock
[container]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/container.lock
[object]
max connections = 2
path = /srv/node/
read only = false
lock file = /var/lock/object.lock
EOF

  systemctl enable rsyncd.service
  systemctl start rsyncd.service


  yum -y install openstack-swift-account openstack-swift-container openstack-swift-object


  if [ -f /root/LocalRepo/Packages/account-server.conf-sample ]; then
     cp /root/LocalRepo/Packages/account-server.conf-sample /etc/swift/account-server.conf 
     cp /root/LocalRepo/Packages/container-server.conf-sample /etc/swift/container-server.conf 
     cp /root/LocalRepo/Packages/object-server.conf-sample /etc/swift/object-server.conf 
     cp /root/LocalRepo/Packages/container-reconciler.conf-sample /etc/swift/container-reconciler.conf 
     cp /root/LocalRepo/Packages/object-expirer.conf-sample /etc/swift/object-expirer.conf 
     echo " " > /root/swift-conf-files-notdl
  else
    curl -o /etc/swift/account-server.conf \
    https://git.openstack.org/cgit/openstack/swift/plain/etc/account-server.conf-sample?h=stable/kilo
 
    curl -o /etc/swift/container-server.conf \
    https://git.openstack.org/cgit/openstack/swift/plain/etc/container-server.conf-sample?h=stable/kilo

    curl -o /etc/swift/object-server.conf \
    https://git.openstack.org/cgit/openstack/swift/plain/etc/object-server.conf-sample?h=stable/kilo

    curl -o /etc/swift/container-reconciler.conf \
    https://git.openstack.org/cgit/openstack/swift/plain/etc/container-reconciler.conf-sample?h=stable/kilo

    curl -o /etc/swift/object-expirer.conf \
    https://git.openstack.org/cgit/openstack/swift/plain/etc/object-expirer.conf-sample?h=stable/kilo
  fi




  for TMPF in /etc/swift/account-server.conf /etc/swift/container-server.conf /etc/swift/object-server.conf
  do  
    susti $TMPF DEFAULT "bind_ip = $MY_MGM_IP"
    susti $TMPF DEFAULT "user = swift"
    susti $TMPF DEFAULT "swift_dir = /etc/swift"
    susti $TMPF DEFAULT "devices = /srv/node"
    susti $TMPF "filter:recon" "recon_cache_path = /var/cache/swift"
  done

  TMPF="/etc/swift/account-server.conf"
   susti $TMPF DEFAULT "bind_port = 6002"
   susti $TMPF "pipeline:main" "pipeline = healthcheck recon account-server"
  TMPF="/etc/swift/container-server.conf" 
   susti $TMPF DEFAULT "bind_port = 6001"
   susti $TMPF "pipeline:main" "pipeline = healthcheck recon container-server"
  TMPF="/etc/swift/object-server.conf"
   susti $TMPF DEFAULT "bind_port = 6000"
   susti $TMPF "pipeline:main" "pipeline = healthcheck recon object-server"
    susti $TMPF "filter:recon" "recon_lock_path = /var/lock"

  chown -R swift:swift /srv/node
  
  mkdir -p /var/cache/swift
  chown -R swift:swift /var/cache/swift
  
}




create_swift_rings (){

  . /tmp/bomsi_{vars,lib_conf}
  #. /tmp/bomsi_vars
  #. /tmp/bomsi_lib_conf


  ## Assuming the amount of disks is the same as the one in install_swift_node 
  DISKS=$(cat ~/FREE_DISKS)


  #Account Ring
  cd /etc/swift
  swift-ring-builder account.builder create 10 3 1
  for IP in $OBJECT_IP
   do 
     for DISK in DISKS; do
       swift-ring-builder account.builder add r1z1-$IP:6002/${DISK}1 100
     done
     #swift-ring-builder account.builder add r1z1-$IP:6002/vdb1 100
     #swift-ring-builder account.builder add r1z1-$IP:6002/vdc1 100
   done

  swift-ring-builder account.builder
  swift-ring-builder account.builder rebalance

  #Cointainer Ring
  cd /etc/swift
  swift-ring-builder container.builder create 10 3 1
  for IP in $OBJECT_IP
   do 
     for DISK in DISKS; do
       swift-ring-builder container.builder add r1z1-$IP:6001/${DISK}1 100
     done
     #swift-ring-builder container.builder add r1z1-$IP:6001/vdb1 100
     #swift-ring-builder container.builder add r1z1-$IP:6001/vdc1 100
   done

  swift-ring-builder container.builder
  swift-ring-builder container.builder rebalance

  #Object Ring
  cd /etc/swift
  swift-ring-builder object.builder create 10 3 1
  for IP in $OBJECT_IP
   do 
     for DISK in DISKS; do
       swift-ring-builder object.builder add r1z1-$IP:6000/${DISK}1 100
     done
     #swift-ring-builder object.builder add r1z1-$IP:6000/vdb1 100
     #swift-ring-builder object.builder add r1z1-$IP:6000/vdc1 100
   done

  swift-ring-builder object.builder
  swift-ring-builder object.builder rebalance
  
  #Copy account.ring.gz, container.ring.gz and object.ring.gz files to /etc/swift/ of each storage node and additional proxy node
  yum -y install sshpass
  for IP in $CONTROLLER_IP #$OBJECT_IP
   do
    sshpass -p $ROOT_PASSWORD ssh -o "StrictHostKeyChecking no" root@$IP hostname
    #sshpass -p $ROOT_PASSWORD ssh-copy-id -o "StrictHostKeyChecking no" root@$IP
    echo $IP
    install-ssh-keys $ROOT_PASSWORD $IP 
    scp /etc/swift/account.ring.gz  root@$IP:/etc/swift/
    scp /etc/swift/container.ring.gz  root@$IP:/etc/swift/
    scp /etc/swift/object.ring.gz  root@$IP:/etc/swift/
   done  

}



finalize_swift_installation () {

  . /tmp/bomsi_vars

  rm -rf /etc/swift/swift.conf
  if [ -f /root/LocalRepo/Packages/account-server.conf-sample ]; then
     cp /root/LocalRepo/Packages/swift.conf-sample /etc/swift/swift.conf 
     echo " " > /root/swift-conf-files-notdl
  else
    curl -o /etc/swift/swift.conf \
    https://git.openstack.org/cgit/openstack/swift/plain/etc/swift.conf-sample?h=stable/kilo
  fi


  export SWIFT_HASH_PREFIX=$(openssl rand -hex 10 |tee token_admin)
  export SWIFT_HASH_SUBFIX=$(openssl rand -hex 10 |tee token_admin)
  echo "SWIFT HASH PATH PREFIX= " $SWIFT_HASH_PREFIX >  ~/SWIFT_HASHES 
  echo "SWIFT HASH PATH SUBFIX= " $SWIFT_HASH_SUBFIX >> ~/SWIFT_HASHES 
 
  TMPF="/etc/swift/swift.conf"
  susti $TMPF "swift-hash" "swift_hash_path_suffix = $SWIFT_HASH_SUBFIX"
  susti $TMPF "swift-hash" "swift_hash_path_prefix = $SWIFT_HASH_PREFIX"
  susti $TMPF "storage-policy:0]" "name = Policy-0"
  susti $TMPF "storage-policy:0]" "default = yes"

  chown -R swift:swift /etc/swift
  
  systemctl enable openstack-swift-proxy.service memcached.service
  systemctl start openstack-swift-proxy.service memcached.service


  #Copy swift.conf files to /etc/swift/ of each storage node and additional proxy node
  yum -y install sshpass
  for IP in $SWIFT_N_IP
   do
    sshpass -p $ROOT_PASSWORD ssh-copy-id -o "StrictHostKeyChecking no" root@$IP
    scp /etc/swift/swift.conf  root@$IP:/etc/swift/
    ssh root@$IP 'chown -R swift:swift /etc/swift' 
   done  

}


start_swift_node (){
  systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service \
  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
  systemctl start openstack-swift-account.service openstack-swift-account-auditor.service \
  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
  systemctl enable openstack-swift-container.service openstack-swift-container-auditor.service \
  openstack-swift-container-replicator.service openstack-swift-container-updater.service
  systemctl start openstack-swift-container.service openstack-swift-container-auditor.service \
  openstack-swift-container-replicator.service openstack-swift-container-updater.service
  systemctl enable openstack-swift-object.service openstack-swift-object-auditor.service \
  openstack-swift-object-replicator.service openstack-swift-object-updater.service
  systemctl start openstack-swift-object.service openstack-swift-object-auditor.service \
  openstack-swift-object-replicator.service openstack-swift-object-updater.service



  service openstack-swift-account start
  service openstack-swift-container start
  service openstack-swift-object start

  #systemctl restart openstack-swift-account openstack-swift-container openstack-swift-object
  #service openstack-swift-start start

  systemctl status openstack-swift-account openstack-swift-container openstack-swift-object
  systemctl status openstack-swift-account.service openstack-swift-account-auditor.service \
    openstack-swift-account-reaper.service openstack-swift-account-replicator.service \
    openstack-swift-container.service openstack-swift-container-auditor.service \
    openstack-swift-container-replicator.service openstack-swift-container-updater.service \
    openstack-swift-object.service openstack-swift-object-auditor.service \
    openstack-swift-object-replicator.service openstack-swift-object-updater.service


  setenforce 0
  service firewalld stop

}







install_heat () {

 . /tmp/bomsi_{vars,lib_conf}
  #. /tmp/bomsi_vars
  #. /tmp/bomsi_lib_conf
 . ~/admin-openrc.sh

  create_sql_user heat $HEAT_DB_PASSWORD

  openstack user create --password $HEAT_PASSWORD --email heat@os.dkrz.de heat
  openstack role add --project service --user heat admin

  openstack role create heat_stack_owner
  openstack role add --project demo --user demo heat_stack_owner

  openstack role create heat_stack_user

  openstack service create --name heat \
  --description "Orchestration" orchestration
  
  openstack endpoint create \
  --publicurl http://controller:8004/v1/%\(tenant_id\)s \
  --internalurl http://controller:8004/v1/%\(tenant_id\)s \
  --adminurl http://controller:8004/v1/%\(tenant_id\)s \
  --region RegionOne \
  orchestration

  openstack endpoint create \
  --publicurl http://controller:8000/v1 \
  --internalurl http://controller:8000/v1 \
  --adminurl http://controller:8000/v1 \
  --region RegionOne \
  cloudformation


  yum -y install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine python-heatclient 

  unalias cp
  rm -rf /etc/heat/heat.conf
  cp -r /usr/share/heat/heat-dist.conf /etc/heat/heat.conf
  chown -R heat:heat /etc/heat/heat.conf


  #PATH=$PATH:/tmp/
  TMPF="/etc/heat/heat.conf"
  susti $TMPF database "connection = mysql://heat:$HEAT_DB_PASSWORD@controller/heat"

  rabbit_parameters $TMPF

   #susti $TMPF DEFAULT "rpc_backend = rabbit"
   #susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
   #susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
   #susti $TMPF oslo_messaging_rabbit "rabbit_password = ${RABBIT_PASS}"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000/v2.0"
  susti $TMPF keystone_authtoken "identity_uri = http://controller:35357"
  susti $TMPF keystone_authtoken "admin_tenant_name = service"
  susti $TMPF keystone_authtoken "admin_user = heat"
  susti $TMPF keystone_authtoken "admin_password = $HEAT_PASSWORD"
  susti $TMPF ec2authtoken "auth_uri = http://controller:5000/v2.0"
  susti $TMPF DEFAULT "heat_metadata_server_url = http://controller:8000"
  susti $TMPF DEFAULT "heat_waitcondition_server_url = http://controller:8000/v1/waitcondition"
  susti $TMPF DEFAULT "stack_domain_admin = heat_domain_admin"
  susti $TMPF DEFAULT "stack_domain_admin_password = $HEAT_DOMAIN_PASS"
  susti $TMPF DEFAULT "stack_user_domain_name = heat_user_domain"
  susti $TMPF DEFAULT "verbose = True"


 BULK_VARS=$( heat-keystone-setup-domain \
  --stack-user-domain-name heat_user_domain \
  --stack-domain-admin heat_domain_admin \
  --stack-domain-admin-password $HEAT_DOMAIN_PASS)

 STACK_USER_DOMAIN_ID=$(echo "$BULK_VARS" |grep stack_user_domain_id |awk -F= '{print $2}')
 STACK_DOMAIN_ADMIN=$(echo "$BULK_VARS" |grep "stack_domain_admin=" |awk -F= '{print $2}')
 STACK_DOMAIN_ADMIN_PASSWORD=$(echo "$BULK_VARS" |grep "stack_domain_admin_password" |awk -F= '{print $2}')

  susti $TMPF DEFAULT "stack_user_domain_id = $STACK_USER_DOMAIN_ID"

  


  su -s /bin/sh -c "heat-manage db_sync" heat

  systemctl enable openstack-heat-api.service openstack-heat-api-cfn.service \
  openstack-heat-engine.service
  systemctl start openstack-heat-api.service openstack-heat-api-cfn.service \
  openstack-heat-engine.service

}

heat_template () {

  . ~/admin-openrc.sh
  . /tmp/bomsi_vars

  cat > /tmp/images/test_stack.yml << EOF
heat_template_version: 2014-10-16
description: A simple server.
 
parameters:
  ImageID:
    type: string
    description: Image use to boot a server
  NetID:
    type: string
    description: Network ID for the server
 
resources:
  server:
    type: OS::Nova::Server
    properties:
      image: { get_param: ImageID }
      flavor: m1.tiny
      networks:
      - network: { get_param: NetID }
 
outputs:
  private_ip:
    description: IP address of the server in the private network
    value: { get_attr: [ server, first_address ] }

EOF

NET_ID=$(nova net-list | awk '/ demo-net / { print $2 }')
heat stack-create -f /tmp/images/test_stack.yml \
  -P "ImageID=cirros-0.3.4-x86_64;NetID=$NET_ID" testStack

heat stack-list

}








install_ceilometer_controller () {

 . /tmp/bomsi_{vars,lib_conf}
  #. /tmp/bomsi_vars
  #. /tmp/bomsi_lib_conf

  yum -y install mongodb-server mongodb

  PATH=$PATH:/tmp/
  TMPF="/etc/mongodb.conf"
  sed -i "s/bind_ip = 127.0.0.1/bind_ip = $CONTROLLER_IP/" $TMPF
  echo "smallfiles = true" >> $TMPF

  systemctl enable mongod.service
  systemctl start mongod.service

 mongo --host controller --eval '
  db = db.getSiblingDB("ceilometer");
  db.addUser({user: "ceilometer",
  pwd: "$CEILOMETER_DB_PASSWORD",
  roles: [ "readWrite", "dbAdmin" ]})' 

#  mongo --host controller --eval '
#   db = db.getSiblingDB("ceilometer");
#   db.createUser({user: "ceilometer",
#   pwd: "Password",
#   roles: [ "readWrite", "dbAdmin" ]})' 

#  mongo --host controller --eval << EOF
#'db = db.getSiblingDB("ceilo");
#db.createUser({user: "ceilometer",
#pwd: "Password",
#roles: [ "readWrite", "dbAdmin" ]})' 
#EOF

MONGOCMD=" \
'db = db.getSiblingDB("ceilometer"); \
db.createUser({user: "ceilometer", \
pwd: "$CEILOMETER_DB_PASSWORD", \
roles: [ "readWrite", "dbAdmin" ]}) '\
"

  mongo --host controller --eval $MONGOCMD
   #Check login
   # mongo ceilometer --host controller -u 'ceilometer' -p 'Password' --eval 'db.version()'
   #for changing the password
   # db.changeUserPassword("ceilometer", "Password")


  #mysql -u root -p$MYSQL_ROOT << EOF
# ' mongo --host controller --eval << EOF'
#db = db.getSiblingDB("ceilometer");
#db.createUser({user: "ceilometer",
#pwd: "$CEILOMETER_DB_PASSWORD",
#roles: [ "readWrite", "dbAdmin" ]})' 
#EOF



  . ~/admin-openrc.sh


  openstack user create --password $CEILOMETER_PASSWORD --email ceilometer@os.dkrz.de ceilometer
  openstack role add --project service --user ceilometer admin

  openstack service create --name ceilometer \
  --description "Telemetry" metering

  openstack endpoint create \
  --publicurl http://controller:8777 \
  --internalurl http://controller:8777 \
  --adminurl http://controller:8777 \
  --region RegionOne \
  metering




   yum -y install openstack-ceilometer-api openstack-ceilometer-collector \
  openstack-ceilometer-notification openstack-ceilometer-central openstack-ceilometer-alarm \
  python-ceilometerclient

  METERING_SECRET=$(openssl rand -hex 10)
  echo $METERING_SECRET > ~/METERING_SECRET

  PATH=$PATH:/tmp/
  TMPF="/etc/ceilometer/ceilometer.conf"
  susti $TMPF database "connection = mongodb://ceilometer:$CEILOMETER_DB_PASSWORD@controller:27017/ceilometer"

  rabbit_parameters $TMPF

   #susti $TMPF DEFAULT "rpc_backend = rabbit"
   #susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
   #susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
   #susti $TMPF oslo_messaging_rabbit "rabbit_password = ${RABBIT_PASS}"
  susti $TMPF DEFAULT "auth_strategy = keystone"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000/v2.0"
  susti $TMPF keystone_authtoken "identity_uri = http://controller:35357"
  susti $TMPF keystone_authtoken "admin_tenant_name = service"
  susti $TMPF keystone_authtoken "admin_user = ceilometer"
  susti $TMPF keystone_authtoken "admin_password = $CEILOMETER_PASSWORD"
  susti $TMPF service_credentials "os_auth_url = http://controller:5000/v2.0"
  susti $TMPF service_credentials "os_username = ceilometer"
  susti $TMPF service_credentials "os_tenant_name = service"
  susti $TMPF service_credentials "os_password = $CEILOMETER_PASSWORD"
  susti $TMPF service_credentials "os_endpoint_type = internalURL"
  susti $TMPF service_credentials "os_region_name = RegionOne"
  susti $TMPF publisher "metering_secret = $METERING_SECRET"
  susti $TMPF DEFAULT "verbose = True"
  
  systemctl enable openstack-ceilometer-api.service openstack-ceilometer-notification.service \
  openstack-ceilometer-central.service openstack-ceilometer-collector.service \
  openstack-ceilometer-alarm-evaluator.service openstack-ceilometer-alarm-notifier.service

  systemctl start openstack-ceilometer-api.service openstack-ceilometer-notification.service \
  openstack-ceilometer-central.service openstack-ceilometer-collector.service \
  openstack-ceilometer-alarm-evaluator.service openstack-ceilometer-alarm-notifier.service 


}



install_ceilometer_compute_agent () {
   # @compute
 . /tmp/bomsi_vars
 
  yum -y install openstack-ceilometer-compute python-ceilometerclient python-pecan
  #PATH=$PATH:/tmp/
  TMPF="/etc/nova/nova.conf"
  susti $TMPF DEFAULT "instance_usage_audit = True"
  susti $TMPF DEFAULT "instance_usage_audit_period = hour"
  susti $TMPF DEFAULT "notify_on_state_change = vm_and_task_state"
  susti $TMPF DEFAULT "notification_driver = messagingv2"

  systemctl restart openstack-nova-compute.service

  TMPF="/etc/ceilometer/ceilometer.conf"
  susti $TMPF publisher "metering_secret = $METERING_SECRET"
  #susti $TMPF publisher "metering_secret = $METERING_SECRET"

  rabbit_parameters $TMPF
 
   #susti $TMPF DEFAULT "rpc_backend = rabbit"
   #susti $TMPF oslo_messaging_rabbit "rabbit_host = controller"
   #susti $TMPF oslo_messaging_rabbit "rabbit_userid = openstack"
   #susti $TMPF oslo_messaging_rabbit "rabbit_password = ${RABBIT_PASS}"
  susti $TMPF keystone_authtoken "auth_uri = http://controller:5000/v2.0"
  susti $TMPF keystone_authtoken "identity_uri = http://controller:35357"
  susti $TMPF keystone_authtoken "admin_tenant_name = service"
  susti $TMPF keystone_authtoken "admin_user = ceilometer"
  susti $TMPF keystone_authtoken "admin_password = $CEILOMETER_PASSWORD"
  susti $TMPF service_credentials "os_auth_url = http://controller:5000/v2.0"
  susti $TMPF service_credentials "os_username = ceilometer"
  susti $TMPF service_credentials "os_tenant_name = service"
  susti $TMPF service_credentials "os_password = $CEILOMETER_PASSWORD"
  susti $TMPF service_credentials "os_endpoint_type = internalURL"
  susti $TMPF service_credentials "os_region_name = $KEYSTONE_REGION"
  susti $TMPF DEFAULT "verbose = True"
  
  systemctl enable openstack-ceilometer-compute.service
  systemctl start openstack-ceilometer-compute.service

}



install_ceilometer_image_service () {
   #@controller
  . /tmp/bomsi_vars

  #PATH=$PATH:/tmp/
  for TMP in /etc/glance/glance-api.conf /etc/glance/glance-registry.conf
   do 
    TMPF=${TMP}
    susti $TMPF DEFAULT "notification_driver = messagingv2"
    susti $TMPF DEFAULT "rpc_backend = rabbit"
    susti $TMPF DEFAULT "rabbit_host = controller"
    susti $TMPF DEFAULT "rabbit_userid = openstack"
    susti $TMPF DEFAULT "rabbit_password = ${RABBIT_PASS}"
   done

  # again
  #TMPF="/etc/glance/glance-registry.conf"
  #susti $TMPF DEFAULT "notification_driver = messaging"
  #susti $TMPF DEFAULT "rpc_backend = rabbit"
  #susti $TMPF DEFAULT "rabbit_host = controller"
  #susti $TMPF DEFAULT "rabbit_userid = openstack"
  #susti $TMPF DEFAULT "rabbit_password = ${RABBIT_PASS}"

  systemctl restart openstack-glance-api.service openstack-glance-registry.service

}


 # @controller && @blockX
configure_ceilometer_block_st () {
  
  PATH=$PATH:/tmp/
  TMPF="/etc/cinder/cinder.conf"
  susti $TMPF DEFAULT "control_exchange = cinder"
  susti $TMPF DEFAULT "notification_driver = messagingv2"

  systemctl restart openstack-cinder-api.service openstack-cinder-scheduler.service
  systemctl restart openstack-cinder-volume.service

} 

configure_ceilometer_object_st (){
  #@object st proxy = controller
 . /tmp/bomsi_vars
 . ~/openrca.sh

 #yum -y install python-ceilometerclient
 
  openstack role create ResellerAdmin
  openstack role add --project service --user ceilometer ResellerAdmin

  #keystone role-create --name ResellerAdmin
  #ROLE=$(keystone role-list | grep "ResellerAdmin" | awk '{print $2}')
  #keystone user-role-add --tenant service --user ceilometer --role $ROLE

  #PATH=$PATH:/tmp/
  TMPF="/etc/swift/proxy-server.conf"
  susti $TMPF filter:keystoneauth "operator_roles = admin,user,ResellerAdmin"
  susti $TMPF pipeline:main "pipeline = authtoken cache healthcheck keystoneauth proxy-logging ceilometer proxy-server"
  susti $TMPF filter:ceilometer "paste.filter_factory = ceilometermiddleware.swift:filter_factory"
  susti $TMPF filter:ceilometer "control_exchange = swift"
  susti $TMPF filter:ceilometer "url = rabbit://openstack:RABBIT_PASS@controller:5672/"
  susti $TMPF filter:ceilometer "driver = messagingv2"
  susti $TMPF filter:ceilometer "topic = notifications"
  susti $TMPF filter:ceilometer "log_level = WARN"

    #susti $TMPF filter:ceilometer "use = egg:ceilometer#swift"
    #susti $TMPF pipeline:main "pipeline = healthcheck cache authtoken keystoneauth ceilometer proxy-server"
    #susti $TMPF filter:keystoneauth "operator_roles = Member,admin,swiftoperator,_member_,ResellerAdmin"

  usermod -a -G ceilometer swift

  pip install ceilometermiddleware
  
  systemctl restart openstack-swift-proxy.service

}



ceilometer_check () {
 . /tmp/bomsi_vars
 . ~/openrca.sh
 
 ceilometer meter-list
 glance image-download "cirros-0.3.3-x86_64" > ~/cirros.img
 ceilometer statistics -m image.download -p 60

}





























